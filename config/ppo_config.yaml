model_params:

  # Training 
  training_steps: 5000      # The number of training steps to perform

  # Save params
  save_freq: 1000
  save_prefix: ppo_model
  trained_model_name: trained_model
  save_replay_buffer: False # PPO does not support saving replay buffer

  # Load model params
  load_model: False
  model_name: ppo_model_5000_steps

  # Logging parameters
  log_folder: PPO_1
  log_interval: 4 # The number of episodes between logs
  reset_num_timesteps: False # If true, will reset the number of timesteps to 0 every training 

  # Use custom policy - Only MlpPolicy is supported (Only used when new model is created)
  use_custom_policy: False
  policy_params:
    net_arch: [400, 300] # List of hidden layer sizes
    activation_fn: relu  # relu, tanh, elu or selu
    features_extractor_class: FlattenExtractor # FlattenExtractor, BaseFeaturesExtractor or CombinedExtractor
    optimizer_class: Adam # Adam, Adadelta, Adagrad, RMSprop or SGD

  # Use SDE
  use_sde: False
  sde_params:
    sde_sample_freq: -1

  # PPO parameters
  ppo_params:
    learning_rate: 0.0003
    n_steps: 100    # The number of steps to run for each environment per update (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel)
    batch_size: 100 # Minibatch size
    n_epochs: 5     # Number of epoch when optimizing the surrogate loss
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    ent_coef: 0.0
    vf_coef: 0.5
    max_grad_norm: 0.5